{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "os.chdir('/Users/JinWei/PycharmProjects/opp_deeplearning/')\n",
    "\n",
    "if os.path.exists('logs'):\n",
    "    for item in os.listdir('logs'):\n",
    "        shutil.rmtree('logs/' + item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n",
      "x_train shape =  (700165, 77)\n",
      "y_train shape = (700165,)\n",
      "Some useful info to get an insight on dataset's shape and normalisation:\n",
      "features shape, labels shape, each features mean, each features standard deviation\n",
      "(120516, 77) (120516,) -0.025180602413949066 0.6998221188698709\n",
      "x_test shape = (120516, 77)\n",
      "y_test shape = (120516,)\n",
      "the dataset is therefore properly normalised, as expected.\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n",
      "segmenting signal...\n",
      "signal segmented.\n",
      "train_x shape = (63650, 23, 77)\n",
      "train_y shape = (63650,)\n",
      "test_x shape = (10955, 23, 77)\n",
      "test_y shape = (10955,)\n",
      "unique test_y [0 1]\n",
      "unique train_y [0 1]\n",
      "test_y[1]= [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "train_y shape(1-hot) = (63650, 18)\n",
      "test_y shape(1-hot) = (10955, 18)\n",
      "len(x_train[0]) 23\n",
      "opp\n",
      "n_inputs len(X_train[0][0]) 77\n",
      "WARNING:tensorflow:From <ipython-input-2-3a3d6488a1d9>:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch: 0,Train accuracy : 0.764398893360161,Train Loss : 0.9172349682500445\n",
      "Epoch: 0,Test accuracy : 0.790050208568573,Test Loss : 0.797264814376831\n",
      "Epoch: 1,Train accuracy : 0.8041844818913481,Train Loss : 0.6719552414553611\n",
      "Epoch: 1,Test accuracy : 0.7689639329910278,Test Loss : 0.7710552215576172\n",
      "Epoch: 2,Train accuracy : 0.8345070422535211,Train Loss : 0.5593571316561393\n",
      "Epoch: 2,Test accuracy : 0.8055682182312012,Test Loss : 0.6860241293907166\n",
      "Epoch: 3,Train accuracy : 0.849786217303823,Train Loss : 0.4929212493732032\n",
      "Epoch: 3,Test accuracy : 0.8131446838378906,Test Loss : 0.6365004181861877\n",
      "Epoch: 4,Train accuracy : 0.8626288983903421,Train Loss : 0.44908100749240437\n",
      "Epoch: 4,Test accuracy : 0.819990873336792,Test Loss : 0.6141509413719177\n",
      "Epoch: 5,Train accuracy : 0.8716046277665996,Train Loss : 0.4160904507918688\n",
      "Epoch: 5,Test accuracy : 0.8356914520263672,Test Loss : 0.5645163059234619\n",
      "Epoch: 6,Train accuracy : 0.8788512323943662,Train Loss : 0.38155652093384845\n",
      "Epoch: 6,Test accuracy : 0.8388863801956177,Test Loss : 0.5548844933509827\n",
      "Epoch: 7,Train accuracy : 0.8878584004024145,Train Loss : 0.35492482182082125\n",
      "Epoch: 7,Test accuracy : 0.8421725034713745,Test Loss : 0.5458685755729675\n",
      "Epoch: 8,Train accuracy : 0.8937217052313883,Train Loss : 0.3320184257794996\n",
      "Epoch: 8,Test accuracy : 0.8503879308700562,Test Loss : 0.5243870615959167\n",
      "Epoch: 9,Train accuracy : 0.9004338531187123,Train Loss : 0.30956454514072584\n",
      "Epoch: 9,Test accuracy : 0.8592423796653748,Test Loss : 0.49279558658599854\n",
      "Epoch: 10,Train accuracy : 0.9058884557344065,Train Loss : 0.28715266151663144\n",
      "Epoch: 10,Test accuracy : 0.8626198172569275,Test Loss : 0.49722203612327576\n",
      "Epoch: 11,Train accuracy : 0.9114688128772636,Train Loss : 0.27232658761063677\n",
      "Epoch: 11,Test accuracy : 0.8655408620834351,Test Loss : 0.494495689868927\n",
      "Epoch: 12,Train accuracy : 0.9147698692152918,Train Loss : 0.25654849159234167\n",
      "Epoch: 12,Test accuracy : 0.860429048538208,Test Loss : 0.5189136862754822\n",
      "Epoch: 13,Train accuracy : 0.9192027162977867,Train Loss : 0.24285470136300513\n",
      "Epoch: 13,Test accuracy : 0.8635326623916626,Test Loss : 0.49115753173828125\n",
      "Epoch: 14,Train accuracy : 0.9233211770623743,Train Loss : 0.22891224801375648\n",
      "Epoch: 14,Test accuracy : 0.8687357306480408,Test Loss : 0.48496076464653015\n",
      "Epoch: 15,Train accuracy : 0.9272981639839034,Train Loss : 0.21485268143282654\n",
      "Epoch: 15,Test accuracy : 0.8678228855133057,Test Loss : 0.49555617570877075\n",
      "Epoch: 16,Train accuracy : 0.9306620975855131,Train Loss : 0.2026564340656119\n",
      "Epoch: 16,Test accuracy : 0.8633500933647156,Test Loss : 0.5138114094734192\n",
      "Epoch: 17,Train accuracy : 0.9339002766599598,Train Loss : 0.19337165769569073\n",
      "Epoch: 17,Test accuracy : 0.8608854413032532,Test Loss : 0.5263715386390686\n",
      "Epoch: 18,Train accuracy : 0.9383645623742455,Train Loss : 0.1781684544684427\n",
      "Epoch: 18,Test accuracy : 0.8631675243377686,Test Loss : 0.5246232748031616\n",
      "Epoch: 19,Train accuracy : 0.9390562122736419,Train Loss : 0.17566500548402483\n",
      "Epoch: 19,Test accuracy : 0.8661798238754272,Test Loss : 0.535645604133606\n",
      "Epoch: 20,Train accuracy : 0.9421529175050302,Train Loss : 0.1652099184135868\n",
      "Epoch: 20,Test accuracy : 0.8680054545402527,Test Loss : 0.5163602232933044\n",
      "Epoch: 21,Train accuracy : 0.9467429577464789,Train Loss : 0.15501994924617427\n",
      "Epoch: 21,Test accuracy : 0.869374692440033,Test Loss : 0.5049898028373718\n",
      "Epoch: 22,Train accuracy : 0.9497296277665996,Train Loss : 0.14619242830422705\n",
      "Epoch: 22,Test accuracy : 0.8677316308021545,Test Loss : 0.534195601940155\n",
      "Epoch: 23,Train accuracy : 0.9502483651911469,Train Loss : 0.14115403409520194\n",
      "Epoch: 23,Test accuracy : 0.8740301132202148,Test Loss : 0.5172123312950134\n",
      "Epoch: 24,Train accuracy : 0.9557344064386318,Train Loss : 0.1301451064643712\n",
      "Epoch: 24,Test accuracy : 0.870378851890564,Test Loss : 0.5411797165870667\n",
      "Epoch: 25,Train accuracy : 0.9568504778672032,Train Loss : 0.126380956131144\n",
      "Epoch: 25,Test accuracy : 0.8760383129119873,Test Loss : 0.5149061679840088\n",
      "Epoch: 26,Train accuracy : 0.958359532193159,Train Loss : 0.11754569827246998\n",
      "Epoch: 26,Test accuracy : 0.8773162961006165,Test Loss : 0.531489372253418\n",
      "Epoch: 27,Train accuracy : 0.9597742706237424,Train Loss : 0.11478079468845998\n",
      "Epoch: 27,Test accuracy : 0.8787767887115479,Test Loss : 0.5400815606117249\n",
      "Epoch: 28,Train accuracy : 0.9640970824949698,Train Loss : 0.10194299511316066\n",
      "Epoch: 28,Test accuracy : 0.8764947652816772,Test Loss : 0.5516588091850281\n",
      "Epoch: 29,Train accuracy : 0.9650402414486922,Train Loss : 0.09826074793894693\n",
      "Epoch: 29,Test accuracy : 0.8733911514282227,Test Loss : 0.5843082070350647\n",
      "Testing Accuracy: 0.87339115\n",
      "Final test accuracy: 0.8733911514282227\n",
      "Best epoch's test accuracy: 0.8787767887115479\n",
      "Validation accuracy: 0.87339115\n",
      "f1_score_weighted 0.8738538239808685\n",
      "f1_score_macro 0.5334490865278442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix: /n [[8475   66   19   27    7   45   29    6   16    2   55    0    5   19\n",
      "    26    3  224   27]\n",
      " [  11   60    0   14    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [   7    1   82    0   12    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    6]\n",
      " [  17    2    1   44    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    7    0]\n",
      " [   3    1    6    1   88    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]\n",
      " [  69    2    0    0    0  136   16    9    1    1    1    0    0    0\n",
      "     0    0    5    9]\n",
      " [  49    0    0    0    0   13   92    0    1    1    0    0    0    0\n",
      "     1    0    0   12]\n",
      " [  58    0    0    0    0    3    2   37    9    2    1    0    0    7\n",
      "     1    0    3    1]\n",
      " [  38    0    1    0    0    0    3    1   38    0    5    0    3    3\n",
      "    11    0    0    1]\n",
      " [  11    0    0    0    0    0    1    1    0   14    9    1    0    0\n",
      "     0    0    0    8]\n",
      " [   4    0    0    0    0    0    0    1    0    0   17    1    3    0\n",
      "     0    0    1    4]\n",
      " [  16    0    0    0    1    0    0    0    1    3    2    8    4    7\n",
      "     2    0    0    0]\n",
      " [   4    0    0    0    0    0    4    1    0    0    5    0    8    2\n",
      "    14    0    0    0]\n",
      " [  10    1    0    0    0    0    0    0    0    0    0    0    1   42\n",
      "    13    0    0    0]\n",
      " [  12    0    0    0    0    0    0    0    0    0    0    0    0    6\n",
      "    43    0    0    0]\n",
      " [  53    2    0    1    0    0    0    0    1    0    0    0    0    0\n",
      "     0   44    9    0]\n",
      " [  99    5    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0  282    0]\n",
      " [  47    0    7    0    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0   58]]\n",
      "\n",
      "Confusion matrix (normalised to % of total test data):\n",
      "[[7.73619385e+01 6.02464616e-01 1.73436791e-01 2.46462807e-01\n",
      "  6.38977587e-02 4.10771310e-01 2.64719307e-01 5.47695085e-02\n",
      "  1.46052033e-01 1.82565041e-02 5.02053857e-01 0.00000000e+00\n",
      "  4.56412584e-02 1.73436791e-01 2.37334549e-01 2.73847543e-02\n",
      "  2.04472828e+00 2.46462807e-01]\n",
      " [1.00410774e-01 5.47695100e-01 0.00000000e+00 1.27795517e-01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [6.38977587e-02 9.12825204e-03 7.48516679e-01 0.00000000e+00\n",
      "  1.09539017e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 5.47695085e-02]\n",
      " [1.55180290e-01 1.82565041e-02 9.12825204e-03 4.01643097e-01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  6.38977587e-02 0.00000000e+00]\n",
      " [2.73847543e-02 9.12825204e-03 5.47695085e-02 9.12825204e-03\n",
      "  8.03286195e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [6.29849434e-01 1.82565041e-02 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.24144232e+00 1.46052033e-01 8.21542665e-02\n",
      "  9.12825204e-03 9.12825204e-03 9.12825204e-03 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  4.56412584e-02 8.21542665e-02]\n",
      " [4.47284341e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.18667275e-01 8.39799166e-01 0.00000000e+00\n",
      "  9.12825204e-03 9.12825204e-03 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 9.12825204e-03 0.00000000e+00\n",
      "  0.00000000e+00 1.09539017e-01]\n",
      " [5.29438615e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 2.73847543e-02 1.82565041e-02 3.37745309e-01\n",
      "  8.21542665e-02 1.82565041e-02 9.12825204e-03 0.00000000e+00\n",
      "  0.00000000e+00 6.38977587e-02 9.12825204e-03 0.00000000e+00\n",
      "  2.73847543e-02 9.12825204e-03]\n",
      " [3.46873581e-01 0.00000000e+00 9.12825204e-03 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 2.73847543e-02 9.12825204e-03\n",
      "  3.46873581e-01 0.00000000e+00 4.56412584e-02 0.00000000e+00\n",
      "  2.73847543e-02 2.73847543e-02 1.00410774e-01 0.00000000e+00\n",
      "  0.00000000e+00 9.12825204e-03]\n",
      " [1.00410774e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 9.12825204e-03 9.12825204e-03\n",
      "  0.00000000e+00 1.27795517e-01 8.21542665e-02 9.12825204e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 7.30260164e-02]\n",
      " [3.65130082e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.12825204e-03\n",
      "  0.00000000e+00 0.00000000e+00 1.55180290e-01 9.12825204e-03\n",
      "  2.73847543e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  9.12825204e-03 3.65130082e-02]\n",
      " [1.46052033e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  9.12825204e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  9.12825204e-03 2.73847543e-02 1.82565041e-02 7.30260164e-02\n",
      "  3.65130082e-02 6.38977587e-02 1.82565041e-02 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [3.65130082e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 3.65130082e-02 9.12825204e-03\n",
      "  0.00000000e+00 0.00000000e+00 4.56412584e-02 0.00000000e+00\n",
      "  7.30260164e-02 1.82565041e-02 1.27795517e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [9.12825167e-02 9.12825204e-03 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  9.12825204e-03 3.83386582e-01 1.18667275e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.09539017e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 5.47695085e-02 3.92514825e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [4.83797342e-01 1.82565041e-02 0.00000000e+00 9.12825204e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  9.12825204e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.01643097e-01\n",
      "  8.21542665e-02 0.00000000e+00]\n",
      " [9.03696954e-01 4.56412584e-02 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  2.57416701e+00 0.00000000e+00]\n",
      " [4.29027855e-01 0.00000000e+00 6.38977587e-02 0.00000000e+00\n",
      "  9.12825204e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 5.29438615e-01]]\n",
      "Note: training and testing data is not equally distributed amongst classes, \n",
      "--- 1911.4725887775421 seconds ---\n",
      "Feed-forward LSTM Opportunity Done\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "import h5py\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def windowz(data, size):\n",
    "    start = 0\n",
    "    while start < len(data):\n",
    "        yield start, start + size\n",
    "        start += (size / 2)\n",
    "        start = int(start)\n",
    "\n",
    "def segment_opp(x_train,y_train,window_size):\n",
    "    segments = np.zeros(((len(x_train)//(window_size//2))-1,window_size,77))\n",
    "    labels = np.zeros(((len(y_train)//(window_size//2))-1))\n",
    "    i_segment = 0\n",
    "    i_label = 0\n",
    "    for (start,end) in windowz(x_train,window_size):\n",
    "        if(len(x_train[start:end]) == window_size):\n",
    "            m = stats.mode(y_train[start:end])\n",
    "            segments[i_segment] = x_train[start:end]\n",
    "            labels[i_label] = m[0]\n",
    "            i_label+=1\n",
    "            i_segment+=1\n",
    "            #print(\"x_start_end\",x_train[start:end])\n",
    "            # segs =  x_train[start:end]\n",
    "            # segments = np.concatenate((segments,segs))\n",
    "            # segments = np.vstack((segments,x_train[start:end]))\n",
    "            # segments = np.vstack([segments,segs])\n",
    "            # segments = np.vstack([segments,x_train[start:end]])\n",
    "            # labels = np.append(labels,stats.mode(y_train[start:end]))\n",
    "    return segments, labels\n",
    "\n",
    "class Config(object):\n",
    "    \"\"\"\n",
    "    define a class to store parameters,\n",
    "    the input should be feature mat of training and testing\n",
    "\n",
    "    Note: it would be more interesting to use a HyperOpt search space:\n",
    "    https://github.com/hyperopt/hyperopt\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_train, X_test, dataset, input_width):\n",
    "        # Input data\n",
    "        self.train_count = len(X_train)  # 7352 training series\n",
    "        self.test_data_count = len(X_test)  # 2947 testing series\n",
    "        self.n_steps = len(X_train[0])  # 128 time_steps per series\n",
    "        print(\"len(x_train[0])\",len(X_train[0]))\n",
    "\n",
    "        # DEFINING THE MODEL\n",
    "        if dataset==\"opp\":\n",
    "            print(\"opp\")\n",
    "            self.input_height = 1\n",
    "            self.input_width = input_width #or 90 for actitracker\n",
    "            self.num_labels = 18  #or 6 for actitracker\n",
    "            self.num_channels = 77 #or 3 for actitracker\n",
    "        else:\n",
    "            print(\"wrong dataset\")\n",
    "\n",
    "\n",
    "        self.learning_rate = 0.001\n",
    "        self.lambda_loss_amount = 0.0015\n",
    "        self.training_epochs = 10\n",
    "        self.batch_size = 64\n",
    "\n",
    "        # LSTM structure\n",
    "        self.n_inputs = len(X_train[0][0])  # Features count is of 9: 3 * 3D sensors features over time\n",
    "        print(\"n_inputs len(X_train[0][0])\",len(X_train[0][0]))\n",
    "        self.n_hidden = 64  # nb of neurons inside the neural network\n",
    "        self.n_classes = self.num_labels  # Final output classes\n",
    "        self.W = {\n",
    "            'hidden': tf.Variable(tf.random_normal([self.n_inputs, self.n_hidden])),\n",
    "            'output': tf.Variable(tf.random_normal([self.n_hidden, self.n_classes]))\n",
    "        }\n",
    "        self.biases = {\n",
    "            'hidden': tf.Variable(tf.random_normal([self.n_hidden], mean=1.0)),\n",
    "            'output': tf.Variable(tf.random_normal([self.n_classes]))\n",
    "        }\n",
    "\n",
    "\n",
    "def LSTM_Network(_X, config):\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    _X = tf.reshape(_X, [-1, config.n_inputs])\n",
    "\n",
    "    # Linear activation\n",
    "    _X = tf.nn.relu(tf.matmul(_X, config.W['hidden']) + config.biases['hidden'])\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(_X, config.n_steps, 0)\n",
    "\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(config.n_hidden, forget_bias=0.5, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(config.n_hidden, forget_bias=0.5, state_is_tuple=True)\n",
    "    # lstm_cell_3 = tf.contrib.rnn.BasicLSTMCell(config.n_hidden, forget_bias=0.5, state_is_tuple=True)\n",
    "    # lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2,lstm_cell_3], state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "    lstm_last_output = outputs[-1]\n",
    "\n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, config.W['output']) + config.biases['output']\n",
    "\n",
    "\n",
    "print(\"starting...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# DATA PREPROCESSING\n",
    "\n",
    "# we start by reading the hdf5 files to a x_train variable, and return the segments to a train_x variable\n",
    "# this applies for the test and validate sets as well.\n",
    "\n",
    "if len(sys.argv)<2:\n",
    "    print(\"Correct use:python script.py <valid_dataset>\")\n",
    "    sys.exit()\n",
    "\n",
    "#dataset = sys.argv[1]\n",
    "dataset = 'opp'\n",
    "if dataset == \"opp\":\n",
    "    path = os.path.join(os.path.expanduser('~'), \n",
    "                        '/Users/m193-hb/PycharmProjects/oppUCIDeepLearning/opportunityUCI-DeepLearning/phase1', \n",
    "                        'opportunity.h5')\n",
    "else:\n",
    "    print(\"Dataset not supported yet\")\n",
    "    sys.exit()\n",
    "\n",
    "f = h5py.File(path, 'r')\n",
    "\n",
    "x_train = f.get('train').get('inputs')[()]\n",
    "y_train = f.get('train').get('targets')[()]\n",
    "\n",
    "x_test = f.get('test').get('inputs')[()]\n",
    "y_test = f.get('test').get('targets')[()]\n",
    "\n",
    "print(\"x_train shape = \", x_train.shape)\n",
    "print(\"y_train shape =\",y_train.shape)\n",
    "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
    "print(\"features shape, labels shape, each features mean, each features standard deviation\")\n",
    "print(x_test.shape, y_test.shape,\n",
    "      np.mean(x_test), np.std(x_test))\n",
    "print(\"x_test shape =\" ,x_test.shape)\n",
    "print(\"y_test shape =\",y_test.shape)\n",
    "print(\"the dataset is therefore properly normalised, as expected.\")\n",
    "\n",
    "\n",
    "print(np.unique(y_train))\n",
    "print(np.unique(y_test))\n",
    "unq = np.unique(y_test)\n",
    "\n",
    "input_width = 23\n",
    "if dataset == \"opp\":\n",
    "    input_width = 23\n",
    "    print(\"segmenting signal...\")\n",
    "    train_x, train_y = segment_opp(x_train,y_train,input_width)\n",
    "    test_x, test_y = segment_opp(x_test,y_test,input_width)\n",
    "    print(\"signal segmented.\")\n",
    "else:\n",
    "    print(\"no correct dataset\")\n",
    "    exit(0)\n",
    "\n",
    "print(\"train_x shape =\",train_x.shape)\n",
    "print(\"train_y shape =\",train_y.shape)\n",
    "print(\"test_x shape =\",test_x.shape)\n",
    "print(\"test_y shape =\",test_y.shape)\n",
    "\n",
    "# One-hot label conversion\n",
    "\n",
    "train = pd.get_dummies(train_y)\n",
    "test = pd.get_dummies(test_y)\n",
    "\n",
    "train, test = train.align(test, join='inner', axis=1) # maybe 'outer' is better\n",
    "\n",
    "train_y = np.asarray(train)\n",
    "test_y = np.asarray(test)\n",
    "\n",
    "\n",
    "print(\"unique test_y\",np.unique(test_y))\n",
    "print(\"unique train_y\",np.unique(train_y))\n",
    "print(\"test_y[1]=\",test_y[1])\n",
    "# test_y = np.asarray(pd.get_dummies(test_y), dtype = np.int8)\n",
    "print(\"train_y shape(1-hot) =\",train_y.shape)\n",
    "print(\"test_y shape(1-hot) =\",test_y.shape)\n",
    "\n",
    "\n",
    "config = Config(train_x, test_x, dataset, input_width)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, config.n_steps, config.n_inputs])\n",
    "Y = tf.placeholder(tf.float32, [None, config.n_classes])\n",
    "\n",
    "pred_Y = LSTM_Network(X, config)\n",
    "\n",
    "# Loss,optimizer,evaluation\n",
    "l2 = config.lambda_loss_amount * \\\n",
    "    sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "# Softmax loss and L2\n",
    "with tf.name_scope('loss'):\n",
    "    cost = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=pred_Y))  # + l2\n",
    "tf.summary.scalar('loss', cost)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(\n",
    "    learning_rate=config.learning_rate).minimize(cost)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_pred = tf.equal(tf.argmax(pred_Y, 1), tf.argmax(Y, 1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "training_epochs = 30\n",
    "loss_over_time_train = np.zeros(training_epochs)\n",
    "accuracy_over_time_train = np.zeros(training_epochs)\n",
    "loss_over_time_test = np.zeros(training_epochs)\n",
    "accuracy_over_time_test = np.zeros(training_epochs)\n",
    "total_batches = train_x.shape[0] // config.batch_size\n",
    "best_accuracy = 0.0\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    # Init summary\n",
    "    train_writer = tf.summary.FileWriter('logs/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('logs/test')\n",
    "    # sess.run(init)\n",
    "    tf.global_variables_initializer().run()\n",
    "    # Keep training until reach max iterations\n",
    "    # cost_history = np.empty(shape=[0],dtype=float)\n",
    "    i = 0\n",
    "    for epoch in range(training_epochs):\n",
    "        cost_history_train = np.empty(shape=[0],dtype=float)\n",
    "        accuracy_history_train = np.empty(shape=[0],dtype=float)\n",
    "        cost_history_test = np.empty(shape=[0],dtype=float)\n",
    "        accuracy_history_test = np.empty(shape=[0],dtype=float)\n",
    "        for step in range(total_batches):\n",
    "            offset = (step * config.batch_size) % (train_y.shape[0] - config.batch_size)\n",
    "            batch_x = train_x[offset:(offset + config.batch_size), :, :]\n",
    "            batch_y = train_y[offset:(offset + config.batch_size), :]\n",
    "            train_summary, _, c, acc = sess.run([merged, optimizer, cost, accuracy], feed_dict={X: batch_x, Y : batch_y})\n",
    "            # Add into train_writer, view in tensorboard\n",
    "            train_writer.add_summary(train_summary, i)\n",
    "\n",
    "            cost_history_train = np.append(cost_history_train,c)\n",
    "            accuracy_history_train = np.append(accuracy_history_train, acc)\n",
    "            i += 1\n",
    "\n",
    "        loss_over_time_train[epoch] = np.mean(cost_history_train)\n",
    "        accuracy_over_time_train[epoch] = np.mean(accuracy_history_train)\n",
    "\n",
    "        print(\"Epoch: {},\".format(epoch) +\n",
    "              \"Train accuracy : {},\".format(accuracy_over_time_train[epoch]) +\n",
    "              \"Train Loss : {}\".format(loss_over_time_train[epoch]))\n",
    "        # after every epoch, we test the model with the test data\n",
    "        test_summary, pred_out, accuracy_out, loss_out = sess.run([merged, pred_Y, accuracy, cost], feed_dict={X: test_x, Y: test_y})\n",
    "        loss_over_time_test[epoch] = loss_out\n",
    "        accuracy_over_time_test[epoch] = accuracy_out\n",
    "        best_accuracy = max(best_accuracy, accuracy_out)\n",
    "        print(\"Epoch: {},\".format(epoch) +\n",
    "              \"Test accuracy : {},\".format(accuracy_out) +\n",
    "              \"Test Loss : {}\".format(loss_out))\n",
    "        best_accuracy = max(best_accuracy, accuracy_out)\n",
    "        # Add into test_writer, view in tensorboard\n",
    "        test_writer.add_summary(test_summary, i)\n",
    "\n",
    "        # Save the info into a file\n",
    "        # merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter('logs/')\n",
    "\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X: test_x, Y: test_y}))\n",
    "    print(\"Final test accuracy: {}\".format(accuracy_out))\n",
    "\n",
    "    # MORE METRICS\n",
    "    print(\"Best epoch's test accuracy: {}\".format(best_accuracy))\n",
    "    # pred_Y is the result of the FF-RNN\n",
    "    y_p = tf.argmax(pred_Y, 1)\n",
    "    val_accuracy, y_pred = sess.run([accuracy, y_p], feed_dict={X:test_x, Y:test_y})\n",
    "    print(\"Validation accuracy:\", val_accuracy)\n",
    "    y_true = np.argmax(test_y,1)\n",
    "\n",
    "    if dataset==\"opp\":\n",
    "        #print(\"f1_score_mean\", metrics.f1_score(y_true, y_pred, average=\"micro\")\n",
    "        print(\"f1_score_weighted\", metrics.f1_score(y_true, y_pred, average=\"weighted\"))\n",
    "        print(\"f1_score_macro\", metrics.f1_score(y_true, y_pred, average=\"macro\"))\n",
    "        # print(\"f1_score_per_class\", metrics.f1_score(y_true, y_pred, average=None)\n",
    "    else:\n",
    "        print(\"wrong dataset\")\n",
    "\n",
    "    plt.figure(1)\n",
    "    #indep_train_axis = np.array(range(config.batch_size, (len(loss_over_time_train)+1)*config.batch_size, config.batch_size))\n",
    "    plt.plot(loss_over_time_train,   \"b--\", label=\"Train losses\")\n",
    "    plt.plot(accuracy_over_time_train, \"g--\", label=\"Train accuracies\")\n",
    "    #indep_test_axis = np.array(range(config.batch_size, (len(loss_over_time_test)+1)*config.batch_size, config.batch_size))\n",
    "    plt.plot(loss_over_time_test,     \"b-\", label=\"Test losses\")\n",
    "    plt.plot(accuracy_over_time_test, \"g-\", label=\"Test accuracies\")\n",
    "\n",
    "    plt.title(\"Training session's progress over iterations\")\n",
    "    plt.legend(shadow=True)\n",
    "    plt.ylabel('Training Progress (Loss or Accuracy values)')\n",
    "    plt.xlabel('Training iteration')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    confusion_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "    print(\"confusion_matrix: /n\", confusion_matrix)\n",
    "    normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
    "    print(\"\")\n",
    "    print(\"Confusion matrix (normalised to % of total test data):\")\n",
    "    print(normalised_confusion_matrix)\n",
    "    print(\"Note: training and testing data is not equally distributed amongst classes, \")\n",
    "    with open('out.txt', 'w') as f:\n",
    "        #print >> f, 'Confusion Matrix: ', confusion_matrix\n",
    "        #print >> f, 'Normalised Confusion Matrix: ', ormalised_confusion_matrix\n",
    "        print('Confusion Matrix:', confusion_matrix, file=f)\n",
    "        print('Normalised Confusion Matrix:', normalised_confusion_matrix, file=f)\n",
    "    # Plot Results:\n",
    "    # plt.figure(4)\n",
    "    # plt.imshow(\n",
    "    #     normalised_confusion_matrix,\n",
    "    #     interpolation='nearest',\n",
    "    #     cmap=plt.cm.rainbow\n",
    "    # )\n",
    "    # plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "    # plt.colorbar()\n",
    "    # tick_marks = np.arange(18)\n",
    "    # plt.xticks(tick_marks, label_map, rotation=90)\n",
    "    # plt.yticks(tick_marks, label_map)\n",
    "    # plt.tight_layout()\n",
    "    # plt.ylabel('True label')\n",
    "    # plt.xlabel('Predicted label')\n",
    "    # plt.show()\n",
    "\n",
    "    #![png](LSTM_files/LSTM_16_0.png)\n",
    "#######################################################################################\n",
    "#### micro- macro- weighted explanation ###############################################\n",
    "#                                                                                     #\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html      #\n",
    "#                                                                                     #\n",
    "# micro :Calculate metrics globally by counting the total true positives,             #\n",
    "# false negatives and false positives.                                                #\n",
    "#                                                                                     #\n",
    "# macro :Calculate metrics for each label, and find their unweighted mean.            #\n",
    "# This does not take label imbalance into account.                                    #\n",
    "#                                                                                     #\n",
    "# weighted :Calculate metrics for each label, and find their average, weighted        #\n",
    "# by support (the number of true instances for each label). This alters macro         #\n",
    "# to account for label imbalance; it can result in an F-score that is not between     #\n",
    "# precision and recall.                                                               #\n",
    "#                                                                                     #\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"Feed-forward LSTM Opportunity Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
